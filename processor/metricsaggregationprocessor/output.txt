./config.go
```
package metricsaggregationprocessor

import (
	"time"
)

type Config struct {
	AggregationPeriod time.Duration `mapstructure:"aggregation_period"`
	MaxStaleness      time.Duration `mapstructure:"max_staleness"`
	// Aggregations contains the metric aggregation settings.
	Aggregations []MetricAggregationConfig `mapstructure:"aggregations"`
}

type MetricAggregationConfig struct {
	// MetricName is the pattern to match metric names against.
	MetricName string `mapstructure:"metric_name"`

	// MatchType determines how the metric name pattern should be matched against metric names.
	MatchType MatchType `mapstructure:"match_type"`

	// NewName is the new name for the metric after aggregation.
	NewName string `mapstructure:"new_name"`

	// AggregationType defines the type of aggregation to be performed.
	AggregationType AggregationType `mapstructure:"aggregation_type"`

	// DataPointAttributes is a list of attributes to be aggregated over.
	DataPointAttributes []string `mapstructure:"data_point_attributes"`

	// KeepOriginal determines whether the original metric is also emitted. This is only applicable when new_name is set.
	KeepOriginal bool `mapstructure:"keep_original"`

	// LowerBound is the lower bound for the histogram buckets. Only applicable when AggregationType is Bucketize.
	LowerBound float64 `mapstructure:"lower_bound,omitempty"`

	// UpperBound is the upper bound for the histogram buckets. Only applicable when AggregationType is Bucketize.
	UpperBound float64 `mapstructure:"upper_bound,omitempty"`

	// BucketCount is the number of buckets between LowerBound and UpperBound. Only applicable when AggregationType is Bucketize.
	BucketCount int `mapstructure:"bucket_count,omitempty"`
}

type MatchType string

const (
	// MatchTypeStrict matches metric names exactly
	Strict MatchType = "strict"
	
	// MatchTypeRegexp matches metric names using a regular expression.
	Regexp  MatchType = "regex"
)

// AggregationType defines the type of aggregation to perform on matching metrics.
type AggregationType string

const (
	// AggregationTypeMin calculates the minimum value of matching metrics.
	Min     AggregationType = "min"
	// AggregationTypeMax calculates the maximum value of matching metrics.
	Max     AggregationType = "max"
	// AggregationTypeCount calculates the count of matching metrics.
	Count   AggregationType = "count"
	// AggregationTypeCount calculates the count of matching metrics.
	Average AggregationType = "average"
	// Bucketize calculates the distribution of matching metrics.
	Bucketize AggregationType = "bucketize"
)






```
./metrics_aggregation_processor.go
```
package metricsaggregationprocessor

import (
	"context"
	"regexp"
	"slices"
	"sync"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/consumer"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.uber.org/zap"
)

type ContextKey string

const (
	CurrentTimeContextKey ContextKey = "currentTime"
)

type metricsAggregationProcessor struct {
	ctx                 context.Context
	cancel              context.CancelFunc
	next                consumer.Metrics
	clock               Clock
	compiledPatterns    map[string]*regexp.Regexp
	logger              *zap.Logger
	config              *Config
	flushedMetrics      pmetric.MetricSlice
	flushedMetricsMutex sync.RWMutex
	windows             map[windowKey]*aggregatedWindow
	windowsMutex        sync.RWMutex
}

func newMetricsAggregationProcessor(cfg *Config, logger *zap.Logger) (*metricsAggregationProcessor, error) {
	compiledPatterns := make(map[string]*regexp.Regexp)
	for _, aggregationConfig := range cfg.Aggregations {
		if aggregationConfig.MatchType == Regexp {
			pattern, err := regexp.Compile(aggregationConfig.MetricName)
			if err != nil {
				logger.Error("Failed to compile regex pattern for metric name", zap.String("metric_name", aggregationConfig.MetricName), zap.Error(err))
				return nil, err
			}
			compiledPatterns[aggregationConfig.MetricName] = pattern
		}
	}

	ctx, cancel := context.WithCancel(context.Background())
	return &metricsAggregationProcessor{
		ctx:              ctx,
		cancel:           cancel,
		config:           cfg,
		logger:           logger,
		windows:          make(map[windowKey]*aggregatedWindow),
		clock:            &realClock{},
		flushedMetrics:   pmetric.NewMetricSlice(),
		compiledPatterns: compiledPatterns,
	}, nil
}

func (m *metricsAggregationProcessor) getAggregationConfigForMetric(metric pmetric.Metric) *MetricAggregationConfig {
	for _, aggregationConfig := range m.config.Aggregations {
		matchesMetricName := false
		switch aggregationConfig.MatchType {
		case Strict:
			if aggregationConfig.MetricName == metric.Name() {
				matchesMetricName = true
			}
		case Regexp:
			pattern, exists := m.compiledPatterns[aggregationConfig.MetricName]
			if exists && pattern.MatchString(metric.Name()) {
				matchesMetricName = true
			}
		}
		if matchesMetricName {
			return &aggregationConfig
		}
	}
	return nil
}

func getMatchingAttributes(aggregationConfig *MetricAggregationConfig, attributes pcommon.Map) pcommon.Map {
	// Get matching keys from the attributes that are in the aggregationConfig.DataPointAttributes
	matchingAttributes := pcommon.NewMap()
	attributes.CopyTo(matchingAttributes)
	matchingAttributes.RemoveIf(func(k string, v pcommon.Value) bool {
		return !slices.Contains(aggregationConfig.DataPointAttributes, k)
	})
	return matchingAttributes
}

func (m *metricsAggregationProcessor) processMetrics(ctx context.Context, md pmetric.Metrics) (pmetric.Metrics, error) {
	currentTime := m.clock.Now()
	rms := md.ResourceMetrics()
	rms.RemoveIf(func(rm pmetric.ResourceMetrics) bool {
		rm.ScopeMetrics().RemoveIf(func(sm pmetric.ScopeMetrics) bool {
			metrics := sm.Metrics()
			metrics.RemoveIf(func(metric pmetric.Metric) bool {
				aggregationConfig := m.getAggregationConfigForMetric(metric)
				if aggregationConfig != nil {
					switch metric.Type() {
					case pmetric.MetricTypeGauge:
						m.aggregateGaugeMetric(metric, aggregationConfig, currentTime)
						// case pmetric.MetricTypeSum:
						// 	m.aggregateSumMetric(metric)
						// case pmetric.MetricTypeHistogram:
						// 	m.aggregateHistogramMetric(metric)
						// }
					}
					if !aggregationConfig.KeepOriginal {
						return true
					}
				}
				return false
			})
			return metrics.Len() == 0
		})
		return rm.ScopeMetrics().Len() == 0
	})

	m.flushedMetricsMutex.Lock()
	if m.flushedMetrics.Len() > 0 {
		rm := md.ResourceMetrics().AppendEmpty()
		sm := rm.ScopeMetrics().AppendEmpty()
		for i := 0; i < m.flushedMetrics.Len(); i++ {
			m.flushedMetrics.At(i).CopyTo(sm.Metrics().AppendEmpty())
		}
		// Clear the flushed metrics
		m.flushedMetrics = pmetric.NewMetricSlice()

	}
	m.flushedMetricsMutex.Unlock()

	return md, nil
}

func (m *metricsAggregationProcessor) Start(ctx context.Context, host component.Host) error {
	go m.startFlushInterval()
	return nil
}

func (m *metricsAggregationProcessor) Shutdown(ctx context.Context) error {
	m.cancel()
	return nil
}

```
./testutils/clock.go
```
package testutils

import (
	"time"
)

type mockClock struct {
    currentTime time.Time
}

func (m *mockClock) Now() time.Time {
    return m.currentTime
}

func (m *mockClock) After(d time.Duration) <-chan time.Time {
    m.currentTime = m.currentTime.Add(d)
    return time.After(0) // immediately return
}

func (m *mockClock) Set(t time.Time) {
    m.currentTime = t
}

func (m *mockClock) Add(d time.Duration) {
    m.currentTime = m.currentTime.Add(d)
}
```
./clock.go
```
package metricsaggregationprocessor

import (
	"time"
)

type Clock interface {
    Now() time.Time
    After(d time.Duration) <-chan time.Time
    // ... any other time-related functions you use
}

type realClock struct{}

func (realClock) Now() time.Time {
    return time.Now()
}

func (realClock) After(d time.Duration) <-chan time.Time {
    return time.After(d)
}

type mockClock struct {
    currentTime time.Time
}

func (m *mockClock) Now() time.Time {
    return m.currentTime
}

func (m *mockClock) After(d time.Duration) <-chan time.Time {
    m.currentTime = m.currentTime.Add(d)
    return time.After(0) // immediately return
}

func (m *mockClock) Set(t time.Time) {
    m.currentTime = t
}

func (m *mockClock) Add(d time.Duration) {
    m.currentTime = m.currentTime.Add(d)
}
```
./gauge.go
```
package metricsaggregationprocessor

import (
	"time"

	"go.opentelemetry.io/collector/pdata/pmetric"
)

// Helper function to get the value from a data point based on its type
func getValue(dp pmetric.NumberDataPoint) float64 {
	switch dp.ValueType() {
	case pmetric.NumberDataPointValueTypeDouble:
		return dp.DoubleValue()
	case pmetric.NumberDataPointValueTypeInt:
		return float64(dp.IntValue())
	default:
		return 0
	}
}

// Helper function to set the value of a data point based on its type
func setValue(dp pmetric.NumberDataPoint, value float64, inType pmetric.NumberDataPointValueType) {
	switch dp.ValueType() {
	case pmetric.NumberDataPointValueTypeDouble:
		dp.SetDoubleValue(value)
	case pmetric.NumberDataPointValueTypeInt:
		dp.SetIntValue(int64(value))
	case pmetric.NumberDataPointValueTypeEmpty:
		switch inType {
		case pmetric.NumberDataPointValueTypeDouble:
			dp.SetDoubleValue(value)
		case pmetric.NumberDataPointValueTypeInt:
			dp.SetIntValue(int64(value))
		default:
			dp.SetDoubleValue(value)
		}
	}
}



func (m *metricsAggregationProcessor) aggregateGaugeMetric(metric pmetric.Metric, aggregationConfig *MetricAggregationConfig, currentTime time.Time) {
	// Get the data points from the metric
	dps := metric.Gauge().DataPoints()

	// Iterate over the data points
	for i := 0; i < dps.Len(); i++ {
		dp := dps.At(i)
		// If the timestamp on this datapoint is before the current time minus max_staleness, skip it
		if dp.Timestamp().AsTime().Before(currentTime.Add(-m.config.MaxStaleness)) {
			continue
		}
		matchingAttributes := getMatchingAttributes(aggregationConfig, dp.Attributes())
		if matchingAttributes.Len() == 0 {
			continue
		}
		relevantWindow := m.getWindowForMetric(metric, matchingAttributes, dp.Timestamp(), aggregationConfig)
		if relevantWindow == nil {
			continue
		}
		switch aggregationConfig.AggregationType {
		// Check the aggregation type and aggregate the data point accordingly
		case Min:
			m.aggregateGaugeMin(relevantWindow, metric, dp)
		case Max:
			m.aggregateGaugeMax(relevantWindow, metric, dp)
		case Count:
			m.aggregateGaugeCount(relevantWindow)
		case Average:
			m.aggregateGaugeAverage(relevantWindow, metric, dp)
		case Bucketize:
			m.aggregateGaugeToHistogram(relevantWindow, metric, dp)
		}
	}

}

func (m *metricsAggregationProcessor) aggregateGaugeMin(window *aggregatedWindow, metric pmetric.Metric, dp pmetric.NumberDataPoint) {
	window.Lock()
	defer window.Unlock()

	if getValue(dp) < getValue(window.metric.Gauge().DataPoints().At(0)) {
		setValue(window.metric.Gauge().DataPoints().At(0), getValue(dp), dp.ValueType())
	}
}

func (m *metricsAggregationProcessor) aggregateGaugeMax(window *aggregatedWindow, metric pmetric.Metric, dp pmetric.NumberDataPoint) {
	window.Lock()
	defer window.Unlock()

	if getValue(dp) > getValue(window.metric.Gauge().DataPoints().At(0)) {
		setValue(window.metric.Gauge().DataPoints().At(0), getValue(dp), dp.ValueType())
	}
}

func (m *metricsAggregationProcessor) aggregateGaugeCount(window *aggregatedWindow) {
	window.Lock()
	defer window.Unlock()

	window.count++
}

func (m *metricsAggregationProcessor) aggregateGaugeAverage(window *aggregatedWindow, metric pmetric.Metric, dp pmetric.NumberDataPoint) {
	window.Lock()
	defer window.Unlock()

	sum := getValue(window.metric.Gauge().DataPoints().At(0)) + getValue(dp)
	setValue(window.metric.Gauge().DataPoints().At(0), sum, dp.ValueType())
	window.count++
}

func (m *metricsAggregationProcessor) aggregateGaugeToHistogram(window *aggregatedWindow, metric pmetric.Metric, dp pmetric.NumberDataPoint) {
	window.Lock()
	defer window.Unlock()
	// Get the data points from the metric
	histDp := window.metric.Histogram().DataPoints().At(0)
	histDp.SetCount(histDp.Count() + 1)
	if histDp.HasSum() {
		histDp.SetSum(histDp.Sum() + getValue(dp))
	} else {
		histDp.SetSum(getValue(dp))
	}
	if histDp.Max() < getValue(dp) || !histDp.HasMax() {
		histDp.SetMax(getValue(dp))
	}
	if histDp.Min() > getValue(dp) || !histDp.HasMin() {
		histDp.SetMin(getValue(dp))
	}
	// Get the bucket index for the value
	bucketIndex := findBucketIndex(getValue(dp), histDp.ExplicitBounds().AsRaw())
	// Increment the bucket count
	histDp.BucketCounts().SetAt(bucketIndex, histDp.BucketCounts().At(bucketIndex) + 1)
}

// Helper function to determine the bucket index for a value
func findBucketIndex(value float64, explicitBounds []float64) int {
	for i, bound := range explicitBounds {
		if value <= bound {
			return i
		}
	}
	return len(explicitBounds) // This will be the index of the last bucket
}

func completeGaugeAggregation(metric pmetric.Metric, aggregationType AggregationType, count int64) {
	switch aggregationType {
	case Count:
		metric.Gauge().DataPoints().At(0).SetIntValue(int64(count))
	case Average:
		setValue(metric.Gauge().DataPoints().At(0), getValue(metric.Gauge().DataPoints().At(0))/float64(count), metric.Gauge().DataPoints().At(0).ValueType())
	}
}
```
./factory.go
```
package metricsaggregationprocessor

import (
	"context"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/consumer"
	"go.opentelemetry.io/collector/processor"
	"go.opentelemetry.io/collector/processor/processorhelper"
)

var processorCapabilities = consumer.Capabilities{MutatesData: true}

const (
	typeStr = "metricsaggregationprocessor"
)

// NewFactory creates a new factory for the metrics aggregation processor.
func NewFactory() processor.Factory {
	return processor.NewFactory(
		typeStr,
		createDefaultConfig,
		processor.WithMetrics(createMetricsProcessor, component.StabilityLevelDevelopment),)
}

// createDefaultConfig creates the default configuration for the processor.
func createDefaultConfig() component.Config {
	return &Config{
	}
}

// createProcessor creates a metrics aggregation processor from the configuration.
func createMetricsProcessor(
	ctx context.Context,
	set processor.CreateSettings,
	cfg component.Config,
	nextConsumer consumer.Metrics,
) (processor.Metrics, error) {
	rCfg := cfg.(*Config)
	// TODO: Validate config
	if err := validateConfiguration(rCfg); err != nil {
		return nil, err
	}

	metricsProcessor, err := newMetricsAggregationProcessor(rCfg, set.Logger)
	if err != nil {
		return nil, err
	}
	
	
	return processorhelper.NewMetricsProcessor(
		ctx,
		set,
		cfg,
		nextConsumer,
		metricsProcessor.processMetrics,
		processorhelper.WithCapabilities(processorCapabilities),
	)
	
}

func validateConfiguration(config *Config) error {
	return nil
}
```
./metadata/generated_status.go
```
// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"go.opentelemetry.io/collector/component"
)

const (
	Type             = "metricstransform"
	MetricsStability = component.StabilityLevelDevelopment
)
```
./window.go
```
package metricsaggregationprocessor

import (
	"sort"
	"strings"
	"sync"
	"time"



	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

var baseTime = time.Unix(0, 0) // Unix epoch 

type aggregatedWindow struct {
	sync.RWMutex
	startTime time.Time
	metric    pmetric.Metric
	aggregationType AggregationType
	count     int64 // for average calculation
}

type windowKey struct {
	Name       string
	Attributes string // Serialized representation of attributes
	Type       pmetric.MetricType
	StartTime  pcommon.Timestamp
}

func generateWindowKey(metric pmetric.Metric, attributes pcommon.Map, startTime pcommon.Timestamp) windowKey {
	// Serialize attributes in a consistent manner
	// For simplicity, we'll just join them as key=value pairs, but in practice, you might want a more efficient representation.
	var serializedAttributes []string
	attributes.Range(func(k string, v pcommon.Value) bool {
		serializedAttributes = append(serializedAttributes, k+"="+v.AsString())
		return true
	})
	sort.Strings(serializedAttributes) // Ensure consistent order

	return windowKey{
		Name:       metric.Name(),
		Attributes: strings.Join(serializedAttributes, ","),
		Type:       metric.Type(),
		StartTime:  startTime,
	}
}

func (m *metricsAggregationProcessor) createNewWindow(metric pmetric.Metric, attributes pcommon.Map, startTime pcommon.Timestamp, aggregationConfig *MetricAggregationConfig) *aggregatedWindow {
	windowMetricTimestamp := startTime.AsTime().Add(m.config.AggregationPeriod / 2)
	// create a metric copy from the original metric
	windowMetric := pmetric.NewMetric()
	var windowMetricType pmetric.MetricType
	if aggregationConfig.AggregationType == Bucketize {
		windowMetricType = pmetric.MetricTypeHistogram
	} else {
		windowMetricType = metric.Type()
	}
	switch windowMetricType {
	case pmetric.MetricTypeGauge:
		dp := windowMetric.SetEmptyGauge().DataPoints().AppendEmpty()
		dp.SetTimestamp(pcommon.NewTimestampFromTime(windowMetricTimestamp))
		attributes.CopyTo(dp.Attributes())
	case pmetric.MetricTypeSum:
		dp := windowMetric.SetEmptySum().DataPoints().AppendEmpty()
		dp.SetTimestamp(pcommon.NewTimestampFromTime(windowMetricTimestamp))
		attributes.CopyTo(dp.Attributes())
	case pmetric.MetricTypeHistogram:
		windowMetric.SetEmptyHistogram().SetAggregationTemporality(pmetric.AggregationTemporalityCumulative)
		dp := windowMetric.Histogram().DataPoints().AppendEmpty()
		dp.SetTimestamp(pcommon.NewTimestampFromTime(windowMetricTimestamp))
		attributes.CopyTo(dp.Attributes())
		// If its to bucketize, initialize the scale and bounds
		if aggregationConfig.AggregationType == Bucketize {
			dp.BucketCounts().FromRaw(make([]uint64, aggregationConfig.BucketCount))
			scale := float64(aggregationConfig.UpperBound-aggregationConfig.LowerBound) / float64(aggregationConfig.BucketCount)
			for i := 0; i < aggregationConfig.BucketCount; i++ {
				dp.ExplicitBounds().Append(aggregationConfig.LowerBound + float64(i)*scale)
			}
		}
	}
	if aggregationConfig.NewName != "" {
		windowMetric.SetName(aggregationConfig.NewName)
	}
	window := &aggregatedWindow{
		startTime: startTime.AsTime(),
		metric:    windowMetric,
		aggregationType: aggregationConfig.AggregationType,
	}
	return window
}

func getRoundedStartTime(timestamp pcommon.Timestamp, aggregationPeriod time.Duration) pcommon.Timestamp {
	// Convert the pcommon.Timestamp to time.Time
	timestampTime := timestamp.AsTime()

	// Calculate the number of aggregation periods since the Unix epoch
	periodsSinceEpoch := timestampTime.UnixNano() / int64(aggregationPeriod)

	// Calculate the rounded start time in nanoseconds
	roundedStartTimeNano := periodsSinceEpoch * int64(aggregationPeriod)

	// Convert the rounded start time in nanoseconds back to pcommon.Timestamp
	roundedStartTime := pcommon.NewTimestampFromTime(time.Unix(0, roundedStartTimeNano))

	return roundedStartTime
}

func (m *metricsAggregationProcessor) getWindowForMetric(metric pmetric.Metric, attributes pcommon.Map, timestamp pcommon.Timestamp, aggregationConfig *MetricAggregationConfig) *aggregatedWindow {
	roundedStartTime := getRoundedStartTime(timestamp, m.config.AggregationPeriod)

	windowKey := generateWindowKey(metric, attributes, roundedStartTime)
	m.windowsMutex.Lock()
	defer m.windowsMutex.Unlock()

	// Check if a window for the metric key already exists
	if window, exists := m.windows[windowKey]; exists {
		return window
	}

	newWindow := m.createNewWindow(metric, attributes, roundedStartTime, aggregationConfig)

	// Store the new window in the map and return it
	m.windows[windowKey] = newWindow
	return newWindow

}

func (m *metricsAggregationProcessor) flushWindows() {
	currentTime := m.clock.Now()
	m.windowsMutex.Lock()
	m.flushedMetricsMutex.Lock()
	for key, window := range m.windows {
		if window.startTime.Add(m.config.MaxStaleness).Before(currentTime) {
			window.complete()
			window.metric.CopyTo(m.flushedMetrics.AppendEmpty())
			delete(m.windows, key)
		}
	}
	m.windowsMutex.Unlock()
	m.flushedMetricsMutex.Unlock()
}

func (m *metricsAggregationProcessor) startFlushInterval() {
	delay := m.config.AggregationPeriod / 4
	for {
		select {
		case <-m.ctx.Done():
			return
		case <-m.clock.After(delay):
			m.flushWindows()
		}
	}
}

func (w *aggregatedWindow) complete(){
	w.Lock()
	defer w.Unlock()
	switch w.metric.Type() {
	case pmetric.MetricTypeGauge:
		completeGaugeAggregation(w.metric, w.aggregationType, w.count)	
	}
}
```

